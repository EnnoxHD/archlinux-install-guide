QEMU    |-> emulator and hypervisor
        |-> uses KVM
        |-> Variants
        |   - full-system emulation
        |       - primarily for non-Linux guests
        |       - qemu command e.g. `qemu-system-x86_64`
        |       - target architecture == host architecture --> speedup with: KVM
        |   - usermode emulation
        |       - only for Linux guests
        |       - requires `qemu-arch-extra` to be installed
        |       - see https://wiki.archlinux.org/title/QEMU#Details_on_packages_offered_in_Arch_Linux
        |       - qemu command `qemu-x86_64`
        - Install `qemu` (only `qemu-system-x86_64` architecture)
        - Create a system
            - Hard disk image
                - create
                    - raw: byte-by-byte full pysical drive, for performance
                    - qcow2: virtual hard disk growing, snapshotting feature
                        - e.g. `qemu-img create -f qcow2 image_file 40G`
                    - overlay image file: keep backing image and derive mutations
                - resize
                    - e.g. `qemu-img resize disk_image +10G` to increase the image by 10 GB
                    - in guest system enlarge the partition
                - convert
                    - e.g. `qemu-img convert -f raw -O qcow2 input.img output.qcow2`
            - memory
                - e.g. `-m 2G` parameter for 2 GB RAM
            - boot order
                - e.g. `-boot order=x` where `x` is some letter for a device
                - e.g. `-boot menu=on` for a boot menu
        - Run a system
            - e.g. `qemu-system-x86_64 options disk_image`
                - [all options](https://man.archlinux.org/man/qemu.1)
                - Release mouse pointer from qemu window `Ctrl+Alt+g`
            - enable KVM
                - option `-enable-kvm` (alternatives: `-machine accel=kvm` or `-accel kvm`)
                - cpu model `host` requires KVM
                - check if enabled in VM: in QEMU Monitor `info kvm`
            - enable IOMMU
                - see IOMMU section
                - optional emulation of iommu device (do not use for pci passthrough on intel)
                    - add `-device intel-iommu` for emulation -> diables pci-passthrough on intel
                    - e.g. `qemu-system-x86_64 -enable-kvm -machine q35 -device intel-iommu -cpu host`
        - share data between host and guest
            - /Network
            | - via network service: nfs, smb, nbd, http, ftp, ssh
            | - host at IP `10.0.2.2`
            | - host cannot access services on VMs by default -> use tap networking with qemu
            - /Port forwarding
                - IPv4 only
                - port forwarding from host to guest (e.g. SSH)
                    - :60022 host with :22 on guest -> e.g. `-nic user,hostfwd=tcp::60022-:22`
                    - from host connect with `ssh guest-user@127.0.0.1 -p 60022`
                    - multiple: e.g. `-nic user,hostfwd=tcp::60022-:22,hostfwd=tcp::5900-:5900`
            - /SSHFS can be used to mount the guest's filesystem on the host
                - https://wiki.archlinux.org/title/SSHFS
            - Setup samba share
                |-> qemu allows only a single directory
                |-> use symbolic links instead
                - Install `samba`
                - start the vm with the `smb` option like `-nic user,id=nic0,smb=shared_dir_path`
                - on the guest the share is available on `\\10.0.2.4\qemu`
            - /filesystem passthrough and virtfs (virtio-9p)
                | - https://wiki.qemu.org/Documentation/9psetup
                | - Plan 9 over Virtio (virtio-9p-device), network protocol
                | - Linux guests only
                | - older standard
            -> file sharing via virtio-fs
                | - see https://wiki.archlinux.org/title/QEMU#Host_file_sharing_with_virtiofsd
                | - https://qemu-stsquad.readthedocs.io/en/docs-next/tools/virtiofsd.html
                | - https://libvirt.org/kbase/virtiofs.html
                | - https://virtio-fs.gitlab.io/howto-windows.html
                | - http://www.secfs.net/winfsp/
                | - newer standard
                - host: `virtiofsd --socket-path=/tmp/vhost-fs.sock -o source=/path/to/shared/dir`
                - host: `qemu ... -chardev socket,id=char0,path=/tmp/vhost-fs.sock -device vhost-user-fs-pci,chardev=char0,tag=myfs`
                - guest: `mount -t virtiofs myfs /mnt`
                - DAX (direct access via shared memory)
                - Libvirt configuration (`hugepage`-backed, also `memfd` or 'file-backed' possible)
                    - share ```text
                        <domain>
                            ...
                            <memoryBacking>
                                <hugepages>
                                    <page size='2' unit='M'/>
                                </hugepages>
                                <access mode='shared'/>
                            </memoryBacking>
                            ...
                            <devices>
                                ...
                                <filesystem type='mount' accessmode='passthrough'>
                                    <driver type='virtiofs'/>
                                    <source dir='/host_path'/>
                                    <target dir='mount_tag'/>
                                </filesystem>
                                ...
                            </devices>
                        </domain>
                        ```
                        - memory configuration: https://libvirt.org/kbase/virtiofs.html#other-options-for-vhost-user-memory-setup
                            - `virsh allocpages 2M 8192` -> 16 GB hugepage memory
                            - see https://www.libvirt.org/manpages/virsh.html#allocpages
                - Mounting on Linux guest: `mount -t virtiofs mount_tag /mnt/mount/path`
                - Mounting on Windows guest:
                    | - Video Guide: https://www.youtube.com/watch?v=j1n_QvHD7uc
                    | - https://virtio-fs.gitlab.io/howto-windows.html
                    - Install Windows on a VM
                    - Install WinFsp->Core (http://www.secfs.net/winfsp/)
                    - Install virtio-fs PCI device driver via Device management
                        - Mass Storage Controller -> Update driver -> From Local
                        - choose the viofs driver from virtio-win ISO and continue
                    - Install a `VirtioFsSvc` service on Windows
                        - Copy the /viofs/w10/amd64/* files from the virtio-win ISO to a VM local path, e.g. `C:\virtiofs`
                        - Admin Cmd:
                            - Create a service with: `sc create VirtioFsSvc binpath="C:\virtiofs\virtiofs.exe" start=auto depend="WinFsp.Launcher/VirtioFsDrv" DisplayName="Virtio FS Service"`
                            - Manually start: `sc start VirtioFsSvc`
                            - Configuring the mount letter
                                - use the `-m <Z:>` parameter for changing the mount letter
                                - see https://github.com/virtio-win/kvm-guest-drivers-windows/wiki/VirtIOFS:-A-shared-file-system
                                - or change it in the Registry via `HKLM\Software\VirtIO-FS\MountPoint`
            - /Mounting a partition of the guest on the host
            - /Using an entire physical disk device inside the VM
        - networking
            | - https://wiki.archlinux.org/title/QEMU#Networking
            | - use virtio / tap and bridges for better performance, not user-mode networking or vde
            - For Windows install the virtio drivers (https://wiki.archlinux.org/title/QEMU#Installing_virtio_drivers)
            - using virtio for networking: ```text
                <interface type='network'>
                    ...
                    <model type='virtio' />
                </interface>
                ```
            - tap networking with virtio: `-device virtio-net,netdev=network0 -netdev tap,id=network0,ifname=tap0,script=no,downscript=no`
            - vhost
                | - in-kernel virtio devices for KVM
                | - vhost-net driver emulates virtio-net in the host kernel
                | - data in kernel, feature negotiation in userspace together with QEMU
                | - partial emulation of a virtio PCI adapter
                | - experimental: vhost-blk, vhost-scsi
                - check if kernel module is running: `lsmod | grep vhost_net`
                - evtl. /etc/modulesload.d/vhost_net.conf: `vhost_net` if not loaded at boot time (or kernel parameter)
                - add `...,vhost=on` for additional performance
            - "Host-only networking"
                - create a bridge (here with iproute2, also possible with https://wiki.archlinux.org/title/Systemd-networkd#Bridge_interface)
                    - Create: `ip link add name br0 type bridge`
                    - `ip link set br0 up`
                    - `ip link set eth0 up`
                    - `ip link set eth0 master br0`
                    - Show all bridges: `bridge link`
                    - Add IP: `ip address add dev br0 192.168.66.66/24`
                    - Remove: `ip link set eth0 nomaster`
                    - `ip link set eth0 down`
                    - `ip link delete br0 type bridge`
                - QEMU Bridge Helper: https://wiki.qemu.org/Features/HelperNetworking
                - in `/etc/qemu/bridge.conf` type `allow br0`
                - check `/etc/qemu/` has `755` permissions
                - use the bridge with e.g. `-nic bridge,br=br0,model=virtio-net-pci`
            - simple NAT over WLAN interface
              ```xml
              <network>
                  <name>default</name>
                  <uuid>c5afe893-c0cb-468f-9a84-e5f0ca9e2f14</uuid>
                  <forward dev='wlan0' mode='nat'>
                      <interface dev='wlan0'/>
                  </forward>
                  <bridge name='virbr0' stp='on' delay='0'/>
                  <mac address='52:54:00:62:c7:2d'/>
                  <ip address='192.168.122.1' netmask='255.255.255.0'>
                      <dhcp>
                          <range start='192.168.122.2' end='192.168.122.254'/>
                      </dhcp>
                  </ip>
              </network>
              ```
        - drive io with virtio
            - For Windows install the virtio drivers (https://wiki.archlinux.org/title/QEMU#Installing_virtio_drivers)
            - using disk io via virtio: ```text
                <disk type='...' device='disk'>
                    ...
                    <target dev='vda' bus='virtio'/>
                </disk>
                ```
            - remove any <address .../> in there to allow correct regenerating this entries

OVMF

IOMMU
    -> kernel param `intel_iommu=on` for remapping io
    -> https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF#Setting_up_IOMMU
    -> https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF#Isolating_the_GPU
    -> via OVMF (UEFI)
    - PCI passthrough
        -> needs KVM and IOMMU support of CPU/Motherboard
        - Enable IOMMU
            - edit /etc/default/grub and add to GRUB_CMD_LINE_DEFAULT
            - ... intel_iommu=on iommu=pt
            - enables IOMMU on intel platform (for AMD it is detected automatically)
            - prevents Linux from touching devices that cannot be passed through
            - Confirm
                - after reboot confirm with `dmesg | grep -i -e DMAR -e IOMMU`
            - Validating groups
                - use the script `sudo iommugroups.sh`
                -   ```bash
                    #!/bin/bash
                    shopt -s nullglob
                    for g in `find /sys/kernel/iommu_groups/* -maxdepth 0 -type d | sort -V`; do
                        echo "IOMMU Group ${g##*/}:"
                        for d in $g/devices/*; do
                            echo -e "\t$(lspci -nns ${d##*/})"
                        done;
                    done;
                    ```
                - unisolated CPU-based PCIe slot
                    - an additional CPU PCIe related device may appear in the same group as the device you are targetting,
                        but it's okay as long as the main device is isolated without other devices in the group besides the CPU PCIe device
                    - example: ```text
                        IOMMU Group 1:
	                        00:01.0 PCI bridge: Intel Corporation Xeon E3-1200 v2/3rd Gen Core processor PCI Express Root Port (rev 09)
	                        01:00.0 VGA compatible controller: NVIDIA Corporation GM107 [GeForce GTX 750] (rev a2)
	                        01:00.1 Audio device: NVIDIA Corporation Device 0fbc (rev a1)
                            ```
                    - otherwise the [ACS override patch](https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF#Bypassing_the_IOMMU_groups_(ACS_override_patch)) may help
        - GPU passthrough
            - enable IOMMU
            - Isolate the GPU
                - Bind vfio-pci via device id
                    - edit /etc/default/grub and add to GRUB_CMD_LINE_DEFAULT
                    - vfio-pci.ids=10de:1b06,10de:10ef
                - Load vfio-pci early
                    - edit /etc/mkinitcpio.conf, preceeding possible graphics drivers
                        - if using early mode setting use: https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF#Binding_vfio-pci_via_device_ID
                    - add modules: `MODULES=(... vfio_pci vfio vfio_iommu_type1 vfio_virqfd ...)`
                    - ensure `HOOKS=(... modconf ...)` is present
                    - regenerate initramfs with `sudo mkinitcpio -p linux`
                - Verify
                    - `dmesg | grep -i vfio`
                    - `lspci -nnk -d 10de:1b06` and `lspci -nnk -d 10de:10ef`
            - isolate from Xorg server
                - edit /etc/X11/xorg.conf
                - in the [Section "ServerLayout"](https://man.archlinux.org/man/extra/xorg-server/xorg.conf.5.en#SERVERLAYOUT_SECTION)
                    - add the `Option "IsolateDevice" "PCI:1:0:0"` according to the guest GPUs IOMMU group/device/function
                - in the [Section "Device"](https://man.archlinux.org/man/extra/xorg-server/xorg.conf.5.en#DEVICE_SECTION)
                    - add the `Option "BusID" "PCI:5:0:0"` according to the host GPUs IOMMU group/device/function
            - additional configuration for Xorg server
                - edit /etc/X11/xorg.conf
                - `Section "Screen"`
                    - add e.g. `Option "nvidiaXineramaInfoOrder" "DFP-0,DFP-2"` (https://manpages.debian.org/stretch/nvidia-xconfig/nvidia-xconfig.1.en.html)
                    - add e.g. `Option "metamodes" "DFP-2: nvidia-auto-select +0+0 {Rotation=left, ForceCompositionPipeline=On, ForceFullCompositionPipeline=On}, DFP-0: nvidia-auto-select +1080+840 {ForceCompositionPipeline=On, ForceFullCompositionPipeline=On}"`
        - Setting up an VM
            - TODO (https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF#Setting_up_an_OVMF-based_guest_virtual_machine)

Virtio  |-> Para-virtualization
        |-> fast and efficient communication to host devices via guest drivers
        |-> API
        |-> between hypervisor and guest
        |-> List of devices:
        |   - network (virtio-net)
        |   - block (virtio-blk)
        |   - controller (virtio-scsi)
        |   - serial (virtio-serial)
        |   - balloon (virtio-balloon)
        - Check for kernel support in guest:
            - General support:
                `zgrep VIRTIO /proc/config.gz`
            - Check for automatic loading:
                `lsmod | grep virtio`

Libvirt |-> Virtual machine management
    | - XML Schema: https://libvirt.org/format.html
    | - https://wiki.archlinux.org/title/Libvirt
    - start libvirtd.service and virtlogd.service / enable libvirtd.service only
    - Audio support (https://libvirt.org/formatdomain.html#audio-backends)
        - via pulseaudio (https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF#Passing_audio_from_virtual_machine_to_host_via_PulseAudio)
            - edit /etc/libvirt/qemu.conf
            - `user = "example"`, use your username instead
            - change VM configuration
                ```text
                <sound model="ich9">
                    <codec type="micro"/>
                    <audio id="1"/>
                </sound>
                <audio id="1" type="pulseaudio" serverName="/run/user/1000/pulse/native">
                    <input mixingEngine="no"/>
                    <output mixingEngine="no"/>
                </audio>
                ```

VMM (virt-manager)

Looking Glass

Cockpit
